# -*- coding: utf-8 -*-
"""Deep Learning Based Pre-trained Image Detection Architecture and Machine Learning Algorithms for Food Image Calorie Estimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M7cpwV1bha9KTz8CtzOFI_wjBeOMKXXM


# **Deep Learning Based Pre-trained Image Detection Architecture and Machine Learning Algorithms for Food Image Calorie Estimation**


#**2. BACKGROUND**

Obesity is said to happen when an individual’s Body Mass Index (BMI) is more than 30 Kg/m2 (Zheng et al., 2011). This is because of the difference between the amount of calories taken in and the energy expended by the individual. Obesity poses a health risk to people around the world. One great and sure way to combat this is to control our daily
calorie intake. That is, by eating healthier food. Though packaged food may come with the calorie composition as a label on the packaging, not all food served in the restaurant have the calorie content displayed to the customer. This was what motivated some researchers to begin to think of ways of developing algorithms that are able to detect food
and calculate the Calorie Contents using Artificial Intelligence (AI).

In the field of AI, Machine and Deep Learning methods with computer vision techniques have found applications in several fields including food calorie estimation (Pouladzadeh et al., 2014;
Jia et al., 2014; and Sun et al., 2008). Deep Convolutional Neural Network-based
algorithms are able to detect and analyze static food images. This Deep Learning (DL) approach works by taking static food images as inputs. This image is often the top and side view of the static image. After which, the algorithm performs object detection using either of the pre-trained Deep Convolutional Neural Network (CNN) based architectures
via the Transfer Learning method. Examples of some pre-trained Deep CNN are
ResNet50 (He et al., 2016) and AlexNet (Krizhevsky et al., 2017). Next, the algorithm computes the contour of the food and estimates the volume therein. With the volume estimate, the amount of food calories the static image has is obtained.

Today, there are a lot of operational food calorie estimation applications. For
example, we have the Im2Calories application developed by Google research scientist called Kevin Murphy and unveiled at the rework Deep Learning Summit in Boston, USA in 2015 (Myers et al., 2015). The application works by combining visual analysis with
pattern recognition. Its accuracy gets better as it is used. Another recent application is the Azati. The application is a calorie counting application that is able to instantly estimate
the calorie content of any static food image based on analysis. The Foodvisor application is also a recent and powerful application that was launched in 2018 in France. The application can be used on a smartphone to photograph a meal and it identifies the food and computes the calorie count, fat, proteins, carbs, and fibers. Additionally, it includes a
module that lets the user monitor their weight loss or gain with an input of height, weight, gender, and age.

In recent literature, there are numerous studies that have applied DL and computer
vision methods for food calorie estimation. Beginning from the older publication to the latest, we will briefly describe how these authors implemented their method.

For example, Sun et al. proposed an electronic photographic method that uses an image processing algorithm to estimate food portion size and other quantitative information (Sun et al.,2008). The author’s method involves semi-automatic image segmentation, camera calibration, and the estimation of the food’s portion size. Experimental results obtained suggest that the mean error and standard deviation are between 5% and 8%.

Jia et al proposed an accuracy of food portion size estimation using digital pictures they obtained by participants wearing a camera on their chests. The chest-worn electronic device is able to capture images of the food during lunch, which is then semiautomatically processed using the software. The software computes the volume of the food. Results obtained suggest that out of the 50 Western and 50 Asian food images evaluated and compared, the mean relative error between the estimated and actual volume is 2.8% and a standard deviation of 20.4%. Additionally, the computer estimates
achieved less bias than those from three raters who estimated the volume of the food visually.

Pouladzadeh et al. proposed a mobile cloud-based food calorie measurement
system (Pouladzadeh et al., 2014). The food detection algorithm in this study uses an ML cloud-based Support Vector Machine (SVM) training mechanism. It also includes a userfriendly interface that allows a seamless computation of food calories before and after consumption. The user interface allows the capturing of the top and size of the food
image, which is processed further using image segmentation methods to identify the
various ingredient portions present in the food and estimate their sizes. Thereafter, the cloud-based ML SVM algorithm is used to identify the ingredient portions in the images.
After which, the ingredient portion sizes computed earlier are used to compute the mass and subsequently the calories. Experimental results obtained suggest that the cloudbased ML SVM model achieved 99% accuracy in a single food portions classification task.

Regarding Region Proposal Networks (RPNs) that assist object detection
architectures to hypothesize object locations, Ren et al. proposed an RPN that shares full-image convolutional features with the detection algorithm (Ren et al., 2015). This is
obtained by optimizing the RPN and the detection algorithm such as the VGG-16
architecture and Fast R-CNN (Girshick et al., 2015; Simonyan and Zisserman, 2015) in training so that they share convolutional features. The method is evaluated over the PASCAL VOC 2007 detection dataset (Everingham et al., 2007). The mean Average
Precision (mAP) metric is used to evaluate the detection, and experimental results obtained using the fast R-CNN and the VGG-1 architectures suggest that the learned RPN improves region proposal quality and the overall object detection accuracy.

Ege and Yanai proposed a method of estimating food calories using multiple-dish food images over two types of food image datasets (Ege and Yanai, 2017). The method uses the fast R-CNN as the detection algorithm, and an image-based food calorie estimation method to evaluate the food calorie of each detected dish. The two study
dataset include the school lunch dataset with the bounding box and the UEC FOOD-100, a Japanese food photo dataset. The experiment in the study is performed by training the faster R-CNN and the RPN modules using the approximate joint training method. For evaluation, the mean Average Precision (mAP) metric is used. Results obtained suggest the model performed with high accuracy.

Lu et al. proposed an AI-based system for a dietary assessment called goFOODTM
(Lu et al., 2020). The application is able to calculate the calorie content of a meal along with other macronutrients using either images or short videos taken from the user’s smartphone. For the detection, segmentation, and recognition of food images and short videos, the application uses Deep Neural networks based on the Mask-R CNN framework
and Inception V3 (Szegedy et al., 2016) architecture. For food volume estimation, the application reconstructs the food 3D model using gravity data from the smartphone’s Inertia Measurement Unit (IMU). To estimate the nutrients in the meal, the application combines an online nutritional database with the estimated food volume. The experiment is performed over two food image databases including MADiMa, which is a centralEuropean style meal, and a McDonald’s international fast food database. Experimental results suggest that the application performed accurately on the fast food database.

Lu et al. proposed an AI-based system to access the nutrient intake of hospital
patients (Lu et al., 2020). The system works by processing the RGB depth of the image pairs obtained by the user before and after meal consumption. For performing food image segmentation, the system uses a novel multi-task contextual network. For food recognition, the system uses a few-shot learning-based classifier model trained via metalearning. For the estimation of consumed food volume, the system uses a 3D surface construction algorithm. Evaluation of the system’s performance over the study’s nutrient
intake is greater than 0.91 correlated to the ground truth.

The efficient training of any Deep CNN consumes a lot of image datasets, requiring techniques such as data augmentation. The data augmentation technique is able to prevent overfitting as it increases the size of a limited model training dataset. The technique is able to increase the amount of data by creating randomized variations and
unique transformations of the original image dataset (Raschka et al., 2022). This can be achieved by flipping, cropping, resizing, etc., the image dataset. Models, such as the
Generative Adversarial Network (GAN) proposed by Goodfellow et al., 2014 is able to perform data augmentation efficiently. Several studies have applied this method in various applications including Kuanar et al., 2019 and Antoniou et al., 2018.

From the literature reviewed above, none of the studies have applied the state-of-the-art Deep CNN-based image detection algorithm such as the fast R-CNN architecture together with ML-based algortihms in food calorie estimation. In this study, we therefore propose a Deep Learning Based Pre-trained Image Detection Architecture and Machine Learning Algorithms for Food Image Calorie Estimation.

#**3. Aim & Objectives**

The main aim of the proposed study is to develop a Deep Learning Based Pre-trained Image Detection Architecture and Machine Learning Algorithms for Food Image Calorie Estimation. The objectives to achieve this aim are to:

a. Review current literature on AI, including Deep and Machine Laerning techniques applied to food calorie estimation;

b. Review the available food database built for food recognition tasks, food
detection/segmentation, and food volume estimation tasks;

c. Select the readily available and most current food database from (b) above, and prepare the data for food calorie estimation;

d. Use the PYTHON TensorFlow Object Detection libraries to compile and build the Fast Deep CNN-based
classifier architecture for object detection task;

e. Obtain relevant information from the detected images, then use them to train, test, and evaluate three ML-based Regressors on image volume, and calorie estimation tasks; and

f. Evaluate the performance of the application using the MAE, MSE, RMSE, and MAPE metrics.


##**Food Database**

Food databases consist of food recognition database, food detection/segmentation database, and food volume estimation database. Food recognition databases are used for food recognition tasks and consist of food images that are annotated with image-level labels. Food detection/segmentation databases often require additional annotation of food location on the images as against that of food recognition database. However, for food volume estimation databases, the annotation required are both image-level labels, pixel-level food segmentation map, and food volume ground truth. In this study, and as part of our second stated objectives, we will review briefly below the available food databases that are built for food recognition tasks, food detection/segementation, and food volume estimation tasks.

###**1. UEC FOOD 100 Dataset**

UEC FOOD 100 Dataset contains 100-kind of food photos with each photo having a bounding box indicating the location of the food item in the photo. It was developed and intended for use in Japan by (Matsuda et al., 2012). The dataset is built to implement a practical food recognition system.

###**2. FOOD 101 Dataset**

FOOD-101 Dataset is a publicly available dataset containing only standardized fast food images taken under laboratory conditions and created by (Bossard et al., 2014). It was created by collecting novel real-world food dataset from a website (foodspotting.com) that allows taking photos of images fo food while eating and uploading it online. The images are all rescaled to 512 pixels, and include diverse, visually, and semantically similar food classes such as Apple pie, waffles, omelette, etc.

###**3. UNIMIB2016 FOOD DATASET**

The UNIMIB2016 FOOD Dataset is one of the dataset collected from a real canteen environment by (Ciocca et al., 2017). It includes a total of 1027 tray images, 73 food categories, and a total of 3616 food instances. All images are annotated and contain multiple foods with accurate segementation. This allows the ease of working on food segementation and quantity estimation methods.

###**4. MULTIMEDIA FOOD DATABASE**

This new multimedia food database contains RGB images pf meals in addition to their corresponding depth maps, 3D models, segementation and recognition maps, weights, and volumes. It is created by (Allegra et al., 2017). The dataset is acquired in a controlled laboratory environment with both constrained and unconstrained set-ups. 80 served Central Europeans meals are involved in the dataset creation.

###**5. ECUSTFD DATASET**

The ECUSTFD Dataset is created by Liang and Li, 2018. It consists of 19 kinds of food with annotations, volume, and mass records, including tomato, sachima, qiwi, plum, pear, peach, orange, moon cake, mix, mango, litchi, the lemon, grapes, fired_dough_twist, eggs, donuts, bun, bread, banana, and apple. Each pair of the images contains a top view and a side view, and a 1 CNY coin used as the calibration object.

###**6. FOODX-251 DATASET**

The FOODX-251 Dataset is one with a fine-grained (prepared) food categories and 158k images obtained from the web. It was created by (Kaur et al., 2019). The dataset includes training, validation, and testing sets. 118k images are set aside for training using the datset, 12k images are set aside for validation, and 28k images are set aside for testing. Examples of food classes in the dataset that are fine-grained are cakes, sandwiches, puddlings, soups and pastas, etc.

###**7. RECIPE 1M+ DATASET**

This is one of the newest large-scale food image dataset with over 1 million cooking recipes and 13 million food images created by Marin et al., 2021. It claims to be the largest publicly available cooking recipe dataset in the world. The dataset is collected by scraping over 24 popular website for cooking.

Given the various food databases reviewed above, coupled with limited computing resources, in this study, we will build and train our deep learning CNN-based architectures for food calorie estimation using the FOODX-251 dataset that is created by (Kaur et al., 2019). In the following sections, we will obtain this dataset from kaggle website and perform exploratory data analysis.

#**8. FRUITS 360 DATASET**

A high-quality, dataset of images containing fruits. The following fruits are included: Apples - (different varieties: Golden, Golden-Red, Granny Smith, Red, Red Delicious), Apricot, Avocado, Avocado ripe, Banana (Yellow, Red), Cactus fruit, Carambula, Cherry, Clementine, Cocos, Dates, Granadilla, Grape (Pink, White, White2), Grapefruit (Pink, White), Guava, Huckleberry, Kiwi, Kaki, Kumsquats, Lemon (normal, Meyer), Lime, Litchi, Mandarine, Mango, Maracuja, Nectarine, Orange, Papaya, Passion fruit, Peach, Pepino, Pear (different varieties, Abate, Monster, Williams), Pineapple, Pitahaya Red, Plum, Pomegranate, Quince, Raspberry, Salak, Strawberry, Tamarillo, Tangelo.

**Dataset properties**

Training set size: 28736 images.

Validation set size: 9673 images.

Number of classes: 60 (fruits).

Image size: 100x100 pixels.

The dataset was created by [23] following the procedures below:

Fruits were planted in the shaft of a low speed motor (3 rpm) and a short movie of 20 seconds was recorded.

A Logitech C920 camera was used for filming the fruits. This is one of the best webcams available.

Behind the fruits we placed a white sheet of paper as background.

However due to the variations in the lighting conditions, the background was not uniform and we wrote a dedicated algorithm which extract the fruit from the background. This algorithm is of flood fill type: we start from each edge of the image and we mark all pixels there, then we mark all pixels found in the neighborhood of the already marked pixels for which the distance between colors is less than a prescribed value. We repeat the previous step until no more pixels can be marked.

All marked pixels are considered as being background (which is then filled with white) and the rest of pixels are considered as belonging to the object.

The maximum value for the distance between 2 neighbor pixels is a parameter of the algorithm and is set (by trial and error) for each movie.

#**9. FOODD DATASET**

**FooDD:**

It is a Food Detection Dataset of 3000 images created by [25]. The images offer variety of food photos taken from different cameras with different illuminations. In this dataset, careful attention was placed in generating the food images characteristics pertained to camera type, shooting angle,
and illumination variations. A strong feature of the dataset is the good distribution
of single and mixed food images. Thus, it can be used to facilitate testing and
benchmarking of various food detection algorithms.

In the dataset, the images are divided into 6 categories considering the capturing
device, background, and lighting condition. Each category contains more
than 100 images.
Note that the thumb is used as a
calibration means to determine the size of the food items in the image.

###**First Mount the Google Drive**
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""###**Check if GPU is connected and display its Details**"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

"""###**Download and Import Libraries and Dependencies**"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np

import os
import os.path as osp
import sys
import pandas as pd
import numpy as np
from os import makedirs
import typing
import random
import pdb
from time import time
import json
import re
import functools
import math
import requests
import datetime
from datetime import datetime
from datetime import date
import urllib.request
import inspect
import logging
import urllib
import zipfile
import copy
import tqdm
from tqdm.notebook import tqdm
import os.path as osp
from math import ceil
import IPython
import IPython.display
import requests
from collections import OrderedDict
from time import time
import PIL # image processing
from IPython.display import Image

import six.moves.urllib as urllib
import tarfile
import tensorflow as tf

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt

# Helper function for visualization.
# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from matplotlib import colors
import matplotlib.lines as mlines
from matplotlib import pyplot
import seaborn as sns
sns.set()
sns.set_style('whitegrid')
from matplotlib.pyplot import figure
import plotly.graph_objects as go
import io
from sklearn.cluster import KMeans
from sklearn.metrics.cluster import (v_measure_score, homogeneity_score, completeness_score)
from sklearn.metrics import roc_auc_score
from sklearn.decomposition import PCA
from sklearn.model_selection import cross_val_score
from typing import Optional, List, Union
from collections import defaultdict
from itertools import repeat
from collections import abc as container_abcs
import matplotlib as mpl
import mpl_toolkits
mpl_toolkits.__path__.append('/usr/lib/python3.7/dist-packages/mpl_toolkits/')
from typing import Collection, Optional, Union
from tqdm import tqdm
import community as louvain
from tqdm import tqdm
import tqdm.notebook # progress bar for jupyter notebook
import time
from time import time
from zipfile import ZipFile
import shutil

from random import shuffle
from sklearn.utils import shuffle

from sklearn.model_selection import train_test_split

from PIL import Image
import cv2

import tensorflow as tf
from glob import glob

from __future__ import print_function, division

import time
import os
import copy

from __future__ import print_function
import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense
from keras.utils.image_utils import img_to_array, array_to_img, load_img

import PIL.Image
from PIL import JpegImagePlugin
from google.colab.patches import cv2_imshow

import six.moves.urllib as urllib
import tarfile
import zipfile

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image
from IPython.display import display

# CNN Model
import tflearn
from tflearn.layers.conv import conv_2d, max_pool_2d
from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.estimator import regression
import tensorflow as tf

from sklearn.model_selection import train_test_split

from torch.utils.data import Dataset , DataLoader
from torchvision import transforms as T
import torchvision
import torch.nn.functional as F
from torch.autograd import Variable

import torchvision.transforms as transforms
import torchvision.models as models
import torch.optim as optim
import torch.nn.modules.loss
import torch.nn.modules.normalization as norm

from PIL import Image
import cv2
import albumentations as A

from torchsummary import summary
import segmentation_models_pytorch as smp

import tensorflow as tf
from glob import glob
import cv2
from skimage.transform import AffineTransform, warp
from skimage import io, img_as_ubyte
from scipy.ndimage import rotate
from albumentations.augmentations.crops.transforms import CropNonEmptyMaskIfExists, RandomCrop
import albumentations as A

from __future__ import print_function, division

from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn
from torchvision import datasets, models, transforms
import time
import copy

cudnn.benchmark = True
plt.ion()   # interactive mode

device =torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""##**Obtain the ECUST Food Dataset (ECUSTFD) from Github Page**

ECUSTFD is a free public food image dataset with 19 types of food. The number of food images is 2978. The number of images and the number of objects for the same type are:

1. apple
2. orange
3. apple
4. banana
5. qiwi
6. mango
7. tomato
8. lemon
9. pear
10. sachima
11. plum
12. peach
13. egg
14. litchi
15. grape
16. fired dough twist
17. bread
18. bun
19. doughnut

To create the Dataset:

Using a smart phone camera, the authors took several groups of images of a single food portion. Each group of images contains a top view and a side view of the food. For each image, there is only one coin as calibration object and no more than two foods in it. If there are two food in the same image, the type of one food is different from another. In this dataset, the original image is resized to 1000 * 1000.

The diameter of the Coin used as calibration object is 25.0mm. Each food item and the calibration object are palced on either a white plate or a red plate before taking the picture. The white plate's diameter is about 20.7cm and its height is about 2.0cm; the red plate's diameter is about 18.7cm and its height is about 2.0cm.

In the ECUSTFD, there are images, annotations of the images, and an excel file with the food volume and quality information.

**Download the Resized ECUST Dataset**
"""

# Commented out IPython magic to ensure Python compatibility.
#uncomment to download the data
#!git clone https://github.com/Liang-yc/ECUSTFD-resized-.git
# %cd ECUSTFD-resized-

"""
**Exploratory Data Analysis (EDA) - ECUSTFD Image Dataset**"""

# Upload the Image Dataset
print(os.listdir("/content/ECUSTFD-resized-/JPEGImages"))

print(len(os.listdir("/content/ECUSTFD-resized-/JPEGImages/test")))

# Upload the Annotation Image Dataset
print(os.listdir("/content/ECUSTFD-resized-/Annotations"))

IMG_MODE = 'RGB'
IMG_SIDE = 1000

def parseFolder(folder, ext):
    filelist = os.listdir(folder)
    imgfiles = []
    for i, file in tqdm.notebook.tqdm(enumerate(filelist), total=len(filelist)):
        if file.endswith(ext):
            imgfiles.append(os.path.join(folder, file))

    # Calling DataFrame constructor on list
    df = pd.DataFrame(data=imgfiles, columns=["filename"])
    return df

def makePatchwork(files, width, height):
    patchwork = PIL.Image.new(IMG_MODE, (IMG_SIDE * width, IMG_SIDE * height))
    matrix = [[(files[y*height+x], x, y) for x in range(width)] for y in range(height)]
    patches = [item for sublist in matrix for item in sublist]
    for (f, x, y) in patches:
        patchwork.paste(PIL.Image.open(f), (x * IMG_SIDE, y * IMG_SIDE))
    return patchwork

# Upload the Image images
image_dataset = os.path.join('/content/ECUSTFD-resized-/JPEGImages')

# parsing folder for JPG files
image_dataset = parseFolder(image_dataset, ".JPG")

print('=============================================================================')
print("Total number of images (128x128px): {}".format(len(image_dataset.index)))
print('=============================================================================')
print(image_dataset.head(10))
print('=============================================================================')

"""**Visualize the Image Dataset**"""

## Display some images:

# sample 100 images from the dataset
sampled = image_dataset.sample(100)

# retrieve the corresponding filenames
files = [ i for i in sampled['filename'] ]

# make a patchwork
patchwork = makePatchwork(files, 10, 10)

# display it inline
plt.figure(figsize=(10, 10))
plt.axis('off')
plt.title("Patchwork of the ECUSTFD Images Dataset", size = 20, fontweight='bold')
plt.imshow(patchwork)
today = date.today()
plt.annotate(f"Bassey Kalu {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(-0.1, -0.05), textcoords='axes fraction',
             color='black')

# save the image
patchwork.save('image_dataset_sampled.jpg')

sampled = sampled
plt.figure(figsize=(10, 10))
for i in range(12):
  img = cv2.imread(sampled.iloc[i]['filename'])
  plt.subplot(3, 4, i + 1)
  plt.suptitle("Sample of the ECUSTFD Images Dataset", size = 20, fontweight='bold')
  plt.imshow(img)

"""**Convert Image Dataset to CSV**"""

train_folder = '/content/ECUSTFD-resized-/JPEGImages/'

files_in_train = sorted(os.listdir(train_folder))
images=[i for i in files_in_train]

df = pd.DataFrame()
df['images']=[train_folder+str(x) for x in images]

# write the DataFrame to a CSV file
df.to_csv('/content/ecustfd.csv', header=None)

print(df.shape)

df.columns

df.info()

df.isnull().sum()

df['images'].value_counts()

"""#**USING THE ECUST FOOD DATASET**

**Git Clone the Application**
"""
#uncomment to download the data
#!git clone https://github.com/HaaaToka/SeeFood.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/SeeFood/

"""##**Prepare the Data for Modeling**

**Function to Clean the Data**
"""

os.mkdir("/content/ECUSTFD-resized-/Dataset/")
import xml.etree.ElementTree as ET

path="/content/ECUSTFD-resized-/Annotations"
for elem in os.listdir(path):
    print(elem)
    inpath = path+"/"+elem
    inputfile = open(inpath,'r')
    out = open('/content/ECUSTFD-resized-/Dataset/'+elem, 'w')

    t=0
    for line in inputfile.readlines():
        if 'owner' in line or t>0:
            t+=1
            if t==4:
                t=0
            continue

        print(line,file=out,end='')

"""**Function to Split the Annotations in .XML into Train and Test Sets**"""

from collections import defaultdict
from math import ceil,floor

# Create the Train and Test Folder in the ANNOTATION Directory:
os.mkdir('/content/ECUSTFD-resized-/Annotations/train')
os.mkdir('/content/ECUSTFD-resized-/Annotations/test')

dicSide = { 'apple':[], 'banana':[], 'bread':[], 'bun':[],
        'doughnut':[], 'egg':[], 'fired':[],
        'grape':[], 'lemon':[], 'litchi':[], 'mango':[],
        'mooncake':[], 'orange':[], 'peach':[], 'pear':[],
        'plum':[], 'qiwi':[], 'sachima':[], 'tomato':[],'mix':[] }
dicTop = { 'apple':[], 'banana':[], 'bread':[], 'bun':[],
        'doughnut':[], 'egg':[], 'fired':[],
        'grape':[], 'lemon':[], 'litchi':[], 'mango':[],
        'mooncake':[], 'orange':[], 'peach':[], 'pear':[],
        'plum':[], 'qiwi':[], 'sachima':[], 'tomato':[],'mix':[] }

c=0
for file in os.listdir('/content/ECUSTFD-resized-/Annotations'):
    if 'xml' in file:
        for key in dicSide.keys():
            if key in file:
                if 'S' in file:
                    dicSide[key].append(file)
                else:
                    dicTop[key].append(file)
                break

k='qiwi'
print(dicSide[k][1])
print(dicSide[k][1].replace('S','T'))

t=0
for k in dicTop.keys():
    t+=len(dicSide[k])+len(dicTop[k])
    print(k,"\n",len(dicSide[k]))
    print(len(dicTop[k]),"\n --------------")
print(t)

for key in dicSide:
    count_food_Side = len(dicSide[key])
    train_size_Side=floor((count_food_Side/10)*9.5)+10**10
    print(count_food_Side,train_size_Side,"SIDE>>>>>>>>")
    count_food_Top = len(dicTop[key])
    train_size_Top=floor((count_food_Top/10)*8.5)
    print(count_food_Top,train_size_Top,"TOP>>>>>>>>")

    if(train_size_Side==train_size_Top):

        for i in range(train_size_Side):
            filename=dicSide[key][i]
            shutil.move("/content/ECUSTFD-resized-/Annotations/"+filename,"/content/ECUSTFD-resized-/Annotations/train/"+filename)
            filename = filename.replace('S','T')
            shutil.move("/content/ECUSTFD-resized-/Annotations/"+filename,"/content/ECUSTFD-resized-/Annotations/train/"+filename)

        for i in range(train_size_Side,count_food_Side):
            filename=dicSide[key][i]
            shutil.move("/content/ECUSTFD-resized-/Annotations/"+filename,"/content/ECUSTFD-resized-/Annotations/test/"+filename)
            filename = filename.replace('S','T')
            shutil.move("/content/ECUSTFD-resized-/Annotations/"+filename,"/content/ECUSTFD-resized-/Annotations/test/"+filename)

    else:

        lis = dicSide[key]
        dcSi = defaultdict(list)
        for elem in lis:
            name = elem.split('S')[0]
            dcSi[name].append(elem)

        print(dcSi.keys())

        lis2 = dicTop[key]
        dcTo = defaultdict(list)
        for elem in lis2:
            name = elem.split('T')[0]
            dcTo[name].append(elem)

        for kk in dcSi.keys():

            count_food_Side = len(dcSi[kk])
            train_size_Side = floor((count_food_Side/10)*9.5)
            print(count_food_Side,train_size_Side,"SIDE>>>>>>>>",kk)
            count_food_Top = len(dcTo[kk])
            train_size_Top=floor((count_food_Top/10)*9.5)
            print(count_food_Top,train_size_Top,"TOP>>>>>>>>",kk)
            print(kk,dcSi[kk])
            print(kk,dcTo[kk],"\n")

            for i in range(train_size_Side):
                filename=dcSi[kk][i]
                shutil.move("/content/ECUSTFD-resized-/Annotations/"+filename, "/content/ECUSTFD-resized-/Annotations/train/"+filename)

            for i in range(train_size_Side,count_food_Side):
                filename=dcSi[kk][i]
                shutil.move("/content/ECUSTFD-resized-/Annotations/"+filename,"/content/ECUSTFD-resized-/Annotations/test/"+filename)

            for i in range(train_size_Top):
                filename = dcTo[kk][i]
                shutil.move("/content/ECUSTFD-resized-/Annotations/"+filename,"/content/ECUSTFD-resized-/Annotations/train/"+filename)

            for i in range(train_size_Top,count_food_Top):
                filename = dcTo[kk][i]
                shutil.move("/content/ECUSTFD-resized-/Annotations/"+filename,"/content/ECUSTFD-resized-/Annotations/test/"+filename)

"""**Function to Split the ECUSTFD into Train and Test Sets**"""

from math import ceil,floor

# Create the Train and Test Folder in the JPEGImages Directory:
os.mkdir('/content/ECUSTFD-resized-/JPEGImages/train')
os.mkdir('/content/ECUSTFD-resized-/JPEGImages/test')

dic = { 'apple':[], 'banana':[], 'bread':[], 'bun':[],
        'doughnut':[], 'egg':[], 'fired':[],
        'grape':[], 'lemon':[], 'litchi':[], 'mango':[],
        'mooncake':[], 'orange':[], 'peach':[], 'pear':[],
        'plum':[], 'qiwi':[], 'sachima':[], 'tomato':[],'mix':[] }

for file in os.listdir('/content/ECUSTFD-resized-/Annotations/test'):
    te = file.split('.')[0]+'.JPG'
    shutil.move("/content/ECUSTFD-resized-/JPEGImages/"+te,"/content/ECUSTFD-resized-/JPEGImages/test/"+te)

for file in os.listdir('/content/ECUSTFD-resized-/Annotations/train'):
    te = file.split('.')[0]+'.JPG'
    shutil.move("/content/ECUSTFD-resized-/JPEGImages/"+te,"/content/ECUSTFD-resized-/JPEGImages/train/"+te)

"""**Function to Plot the Dataset**"""

dicTrain = { 'apple':[], 'banana':[], 'bread':[], 'bun':[],
        'doughnut':[], 'egg':[], 'fired_dough_twist':[],
        'grape':[], 'lemon':[], 'litchi':[], 'mango':[],
        'mooncake':[], 'orange':[], 'peach':[], 'pear':[],
        'plum':[], 'qiwi':[], 'sachima':[], 'tomato':[] }
dicTest = { 'apple':[], 'banana':[], 'bread':[], 'bun':[],
        'doughnut':[], 'egg':[], 'fired_dough_twist':[],
        'grape':[], 'lemon':[], 'litchi':[], 'mango':[],
        'mooncake':[], 'orange':[], 'peach':[], 'pear':[],
        'plum':[], 'qiwi':[], 'sachima':[], 'tomato':[] }

foods = ['apple','egg','lemon','orange','peach','plum','qiwi','tomato','mix',
         ' bread','grape','mooncake','sachima','banana','bun','doughnut',
          'fired_dough_twist','litchi','mango','pear']

mixTest, mixTrain =0, 0
for elem in os.listdir("/content/ECUSTFD-resized-/JPEGImages/test"):
    if 'mix' in elem:
        mixTest+=1
        continue
    for keys in dicTest.keys():
        if keys in elem:
            dicTest[keys].append(elem)
            break

for elem in os.listdir("/content/ECUSTFD-resized-/JPEGImages/train"):
    if 'mix' in elem:
        mixTrain+=1
        continue
    for keys in dicTest.keys():
        if keys in elem:
            dicTrain[keys].append(elem)
            break

print(mixTest, mixTrain)

names=dicTest.keys()

groupCount=len(names)
fig,ax=plt.subplots()
index=np.arange(groupCount)
bar_width = 0.35
opacity = 0.8

countTest=[]
countTrain=[]
for elem in names:
    countTest.append(len(dicTest[elem]))
    countTrain.append(len(dicTrain[elem]))

rects1 = plt.barh(index, countTest, bar_width,
                 alpha=opacity,
                 color='b',
                 label='Test')

rects2 = plt.barh(index + bar_width, countTrain, bar_width,
                 alpha=opacity,
                 color='r',
                 label='Train')

ax.set_yticks(index)
ax.set_yticklabels(names)
ax.invert_yaxis()
ax.set_xlabel('Counts')
ax.set_title("Counts of Used Food")
plt.legend()

plt.tight_layout()
plt.show()

"""##**Perform Object Detection**

**Install and Upload the Required Dependencies for Object Detection**
"""

os.getcwd()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

"""**Get tensorflow/models or cd to parent directory of the repository.**"""

import pathlib

if "models" in pathlib.Path.cwd().parts:
  while "models" in pathlib.Path.cwd().parts:
    os.chdir('..')
elif not pathlib.Path('models').exists():
  #!git clone --depth 1 https://github.com/tensorflow/models

"""**Compile protobufs and install the object_detection package**"""

# Commented out IPython magic to ensure Python compatibility.
# %cd models/research/
#!protoc object_detection/protos/*.proto --python_out=.

# Install TensorFlow Object Detection API.
#!cp object_detection/packages/tf2/setup.py .
#!python -m pip install --use-feature=2020-resolver .

# Commented out IPython magic to ensure Python compatibility.
os.getcwd()
# %cd /content/models/research/

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/models/research/object_detection/packages/tf2
#!pip install .

#!pip install tensorflow-object-detection-api

# Commented out IPython magic to ensure Python compatibility.
# From within TensorFlow/models/research/
os.getcwd()
# %cd /content/models/research
#!python object_detection/builders/model_builder_tf2_test.py

"""**Import the object detection module**"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
os.getcwd()

sys.path.append("/content/models/research/object_detection/utils/")

from object_detection.utils import ops as utils_ops
from object_detection.utils import label_map_util
from object_detection.utils import visualization_utils as vis_util

os.getcwd()

"""**Load Model Function**"""

def load_model(model_name):
  base_url = 'http://download.tensorflow.org/models/object_detection/'
  model_file = model_name + '.tar.gz'
  model_dir = tf.keras.utils.get_file(
    fname=model_name,
    origin=base_url + model_file,
    untar=True)

  model_dir = pathlib.Path(model_dir)/"saved_model"

  model = tf.saved_model.load(str(model_dir))

  return model

"""**Load the Model and label map**"""

sys.path.append("/content/SeeFood/code/prepareData/")

import visualization_utils as vis_util

# patch tf1 into `utils.ops`
utils_ops.tf = tf.compat.v1

# Patch the location of gfile
tf.gfile = tf.io.gfile

# What model to download.
MODEL_NAME = '/content/ECUSTFD-resized-/SeeFood/model/frcnn_inception'

# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'

# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = os.path.join('/content/ECUSTFD-resized-/SeeFood/model/frcnn_inception/frcnn.pbtxt')
# List of the strings that is used to add correct label for each box.
#PATH_TO_LABELS = os.path.join('legacy/data', 'object-detection.pbtxt')

detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.compat.v1.GraphDef()
  with tf.io.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')

category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)

"""**Add path to the images to the TEST_IMAGE_PATHS**"""

def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)

dicSd = {}
dicTp = {}

PATH_TO_TEST_IMAGES_DIR = '/content/ECUSTFD-resized-/JPEGImages/test'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, file) for file in os.listdir(PATH_TO_TEST_IMAGES_DIR) if 'JPG' in file]
#TEST_IMAGE_PATHS=[os.path.join(PATH_TO_TEST_IMAGES_DIR,"fired_dough_twist001T(7).JPG")]
# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)

for file in os.listdir(PATH_TO_TEST_IMAGES_DIR):
    if 'JPG' in file:
        sito = file.split('(')[0][-1]
        cate = file.split('0')[0]
        speci = file.split('(')[0][:-1]
        if cate not in dicSd.keys():
            dicSd[cate]=defaultdict(list)
            dicTp[cate]=defaultdict(list)
print(dicSd)

"""**Add a wrapper function to call the model, and cleanup the outputs**"""

def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.compat.v1.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.compat.v1.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.compat.v1.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.compat.v1.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict

"""**Load the Volume and Weight Information and Display the Length of the Test Dataset**"""

shape = {'ellipsoid':['apple','egg','lemon','orange','peach','plum','qiwi','tomato','mix'],
        'column':['bread','grape','mooncake','sachima'],
        'irregular':['banana','bun','doughnut','fired_dough_twist','litchi','mango','pear']}

def findShape(shapes,exp):
    for k,v in shapes.items():
        for elem in v:
            if exp==elem:
                return k

xls_to_dict={}
xls = pd.ExcelFile("/content/ECUSTFD-resized-/density.xls")

for i in range(20):
    sheetX = xls.parse(i) #2 is the sheet number
    idd = sheetX['id']
    typee = sheetX['type']
    vol = sheetX['volume(mm^3)']
    wei = sheetX['weight(g)']
    if 'mix' in idd[0]:
        xls_to_dict['mix']={}
    else:
        xls_to_dict[typee[0]]={}
    for k in range(len(idd)):
        if 'mix' in idd[0]:
            xls_to_dict['mix'][idd[k]]=[vol[k],wei[k]]
        else:
            xls_to_dict[typee[0]][idd[k]]=[vol[k],wei[k]]

dicSide = defaultdict(list)
dicTop = defaultdict(list)

print(len(TEST_IMAGE_PATHS),"<<<<TEST_IMAGE_PATHS LENGTH")

"""**Perform the Object Detection and Visualize Results**"""

breaker = 0
cnt = 0
whichh = 'mix'

for image_path in TEST_IMAGE_PATHS:
  cnt+=1
  print(cnt, "<><><numero><><>")
  print(image_path)
  if whichh in image_path:
      continue
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)

  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)

  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)

  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(image_np,
                                                     output_dict['detection_boxes'],
                                                     output_dict['detection_classes'],
                                                     output_dict['detection_scores'],
                                                     category_index,
                                                     instance_masks=output_dict.get('detection_masks'),
                                                     use_normalized_coordinates=True,
                                                     line_thickness=8)
  display(Image.fromarray(image_np))
  #plt.figure(figsize=(12, 8))
  #plt.imshow(image_np)
  # Delete the image
  #os.remove(image_path)

vis_util.visualize_boxes_and_labels_on_image_array(image_np,
                                                     output_dict['detection_boxes'],
                                                     output_dict['detection_classes'],
                                                     output_dict['detection_scores'],
                                                     category_index,
                                                     instance_masks=output_dict.get('detection_masks'),
                                                     use_normalized_coordinates=True,
                                                     line_thickness=8)
display(Image.fromarray(image_np))

output_dict['detection_classes']

output_dict['detection_boxes'].shape

output_dict['detection_scores'].shape

category_index

"""##**Perform Volume Estimation**

**Obtain the Detected Images Detailed Information for Volume and Calorie Estimation**
"""

dataset = pd.read_csv('/content/ECUSTFD-resized-/SeeFood/code/volume_estiamtion/mix2csv.csv')
dataset

"""**Perform Exploratory Data Analysis on the Dataset**"""

# Attempt to read the CSV file with exception handling
try:
    df = pd.read_csv('/content/ECUSTFD-resized-/SeeFood/code/volume_estiamtion/mix2csv.csv')
except FileNotFoundError:
    print("File not found. Please make sure the filename is correct and the file is in the current directory.")
    exit(1)
except Exception as e:
    print("An error occurred while reading the file: {}".format(e))
    exit(1)

# The CSV file has been successfully read into a DataFrame
print("File read successfully!")

"""Obtain Statistical Description of the Dataset"""

df.describe().T

print("Summary statistics of the dataset:\n{}".format(df.describe()))

"""**Obtain Basic Information About the Dataset**"""

print("Dataset shape: {}".format(df.shape))
print("Dataset columns: {}".format(list(df.columns)))
print("Dataset data types:\n{}".format(df.dtypes))

"""**Check for Missing values**"""

if df.isnull().values.any():
    print("There are missing values in the dataset.")
    print("Number of missing values in each column:\n{}".format(df.isnull().sum()))
else:
     print("There are no missing values in the dataset.")

"""**Visualize the Correlation Among Dataset Features**"""

# Heatmap of the correlations between numerical variables in the dataset
today = date.today()
try:
    corr = df.select_dtypes(include=['float', 'int']).corr()
    fig, ax = plt.subplots(figsize=(20, 20), facecolor=(1, 1, 1))
    sns.heatmap(corr, annot=True, ax=ax)
    ax.set_title("Heatmap of Correlations Between Numerical Variables", size=15, fontweight='bold')
    plt.xticks(size=5, fontweight='bold')
    plt.yticks(size=5, fontweight='bold')
    plt.annotate(f" {today}.",
                 xy=(1, 1), xycoords='data',
                 xytext=(0.0, -0.10), textcoords='axes fraction',
                 color='black')
    plt.show()

# Handle any exceptions that may occur
except Exception as e:
    print("An error occurred while plotting heatmaps: {}".format(e))
    exit(1)

# Frequency tables for each categorical variable in the dataset
today = date.today()
try:
    for col in df.select_dtypes(include=['float', 'int']):
        freq_table = pd.DataFrame(df[col].value_counts(normalize=True))
        freq_table.reset_index(inplace=True)
        freq_table.columns = [col, 'proportion']
        fig, ax = plt.subplots(figsize=(10, 5), facecolor=(1, 1, 1))
        sns.barplot(x=col, y='proportion', data=freq_table, ax=ax)
        ax.set_xlabel(col, weight='bold').set_fontsize('18')
        ax.set_ylabel('Proportion', weight='bold').set_fontsize('18')
        ax.set_title("Frequency table of {}".format(col), size = 20, fontweight='bold')
        plt.xticks(rotation=90, size = 8, fontweight='bold')
        plt.yticks(size = 12, fontweight='bold')
        plt.annotate(f"{today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0.0, -0.10), textcoords='axes fraction',
             color='black')
        plt.show()

# Handle any exceptions that may occur
except Exception as e:
    print("An error occurred while plotting frequency tables: {}".format(e))
    exit(1)

"""**Prepare the Data for Machine Learning Modeling**"""

dataset = dataset.drop(dataset[dataset['Top_coinForeground_pixel'] < 1000].index)
dataset = dataset.drop(dataset[dataset['Side_coinForeground_pixel'] < 1000].index)
dataset = dataset.drop(dataset[dataset['Top_foodForeground_pixel'] < 1000].index)
dataset = dataset.drop(dataset[dataset['Side_foodForeground_pixel'] < 1000].index)

dataset["top_ratio"] = (dataset['Top_foodHeigth'] * dataset['Top_foodWidth']) / (dataset['Top_coinHeigth'] * dataset['Top_coinWidth'])
dataset['side_ratio'] = (dataset['Side_foodWidth'] * dataset['Side_foodHeigth']) / (dataset['Side_coinHeigth'] * dataset['Side_coinWidth'])
dataset['top_ratio_pixels'] = (dataset['Top_foodForeground_pixel'] / dataset['Top_coinForeground_pixel'])
dataset['side_ratio_pixels'] = (dataset['Side_foodForeground_pixel'] / dataset['Side_coinForeground_pixel'])

dataset['top_coin_ratio'] = 2.5 / ((dataset['Top_coinWidth'] + dataset['Top_coinHeigth']) )
dataset['side_coin_ratio'] = 2.5 / ((dataset['Side_coinWidth'] + dataset['Side_coinHeigth']))

dataset['top_area'] = (dataset['Top_foodHeigth'] * dataset['Top_foodWidth']) * dataset['top_coin_ratio'] * dataset['top_ratio_pixels']
dataset['side_area'] = (dataset['Side_foodHeigth'] * dataset['Side_foodWidth']) * dataset['side_coin_ratio'] * dataset['side_ratio_pixels']
dataset

"""**Split the Dataset into 70% Training and 30% Testing Sets**"""

from sklearn.model_selection import train_test_split
dataset_train,dataset_test = train_test_split(dataset, test_size=0.3, random_state=42)

test_realCalori = dataset_test['real_calorie'].tolist()
test_reDens = dataset_test['realDensity'].tolist()
test_energy = dataset_test['energy'].tolist()
test_avDens = dataset_test['average_density'].tolist()

dicTrain = { 'apple':0, 'banana':0, 'bread':0, 'bun':0,
        'doughnut':0, 'egg':0, 'fired_dough_twist':0,
        'grape':0, 'lemon':0, 'litchi':0, 'mango':0,
        'mooncake':0, 'orange':0, 'peach':0, 'pear':0,
        'plum':0, 'qiwi':0, 'sachima':0, 'tomato':0 }
dicTest = { 'apple':0, 'banana':0, 'bread':0, 'bun':0,
        'doughnut':0, 'egg':0, 'fired_dough_twist':0,
        'grape':0, 'lemon':0, 'litchi':0, 'mango':0,
        'mooncake':0, 'orange':0, 'peach':0, 'pear':0,
        'plum':0, 'qiwi':0, 'sachima':0, 'tomato':0 }

foods=['apple','egg','lemon','orange','peach','plum','qiwi','tomato','mix',
        'bread','grape','mooncake','sachima','banana','bun','doughnut',
        'fired_dough_twist','litchi','mango','pear']

print(dataset_train, "X_train")

for x_t in dataset_train.iterrows():
    #print(x_t,len(x_t[1]), x_t[1][0])
    dicTrain[foods[int(x_t[1][0])]] += 1
print(dicTrain)

for x_t in dataset_test.iterrows():
    #print(x_t,len(x_t[1]), x_t[1][0])
    dicTest[foods[int(x_t[1][0])]] += 1
print(dicTest)

"""**Visulaize the Dataset**"""

plt.rcParams.update({'figure.max_open_warning': 0})

names = dicTest.keys()
groupCount = len(names)

fig, ax = plt.subplots()
index = np.arange(groupCount)
bar_width = 0.35
opacity = 0.8

countTest = []
countTrain = []
for elem in names:
    countTest.append(dicTest[elem])
    countTrain.append(dicTrain[elem])

rects1 = plt.barh(index, countTest, bar_width,
                 alpha=opacity,
                 color='b',
                 label='Test')

rects2 = plt.barh(index + bar_width, countTrain, bar_width,
                 alpha=opacity,
                 color='r',
                 label='Train')

ax.set_yticks(index)
ax.set_yticklabels(names)
ax.invert_yaxis()
ax.set_xlabel('Counts')
ax.set_title("Counts of Used Food")
plt.legend()

plt.tight_layout()
#plt.show()

"""**Prepare X Training Features and y Target Feature**"""

columns_2_drop = ['realVolume', 'realDensity', 'Top_coinWidth',
                            'Top_coinHeigth', 'Side_coinWidth', 'Side_coinHeigth',
                            'real_calorie']

X_train = dataset_train.drop(columns_2_drop, axis=1)
y_train = dataset_train['realVolume']
X_test = dataset_test.drop(columns_2_drop, axis=1)
y_test = dataset_test['realVolume']

"""**Normalize the Dataset**"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

x_train_scaled = scaler.fit_transform(X_train)
X_train = pd.DataFrame(x_train_scaled)

x_test_scaled = scaler.fit_transform(X_test)
X_test = pd.DataFrame(x_test_scaled)

X_train

X_test

"""##**Build the Machine Learning Models to Estimate the Volume and the Calories:**

1. Decision Tree (DT)
2. Random Forest (RF)
3. Support Vector Machine (SVM)

**Define the Evaluation Metrics**
"""

def CORR(pred, true):
    u = ((true - true.mean(0)) * (pred - pred.mean(0))).sum(0)
    d = np.sqrt(((true - true.mean(0)) ** 2 * (pred - pred.mean(0)) ** 2).sum(0))
    return (u / d).mean(-1)

def MAE(pred, true):
    return np.mean(np.abs(pred - true))

def MSE(pred, true):
    return np.mean((pred - true) ** 2)

def RMSE(pred, true):
    return np.sqrt(MSE(pred, true))

def MAPE(pred, true):
    return np.mean(np.abs((pred - true) / true))

def metric(pred, true):
    #corr = CORR(pred, true)
    mae = MAE(pred, true)
    mse = MSE(pred, true)
    rmse = RMSE(pred, true)
    mape = MAPE(pred, true)

    return mae, mse, rmse, mape

"""###**Build the DT Model**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.tree import DecisionTreeRegressor
from math import sqrt

dec_tree_reg = DecisionTreeRegressor(random_state=0)
dec_tree_reg.fit(X_train, y_train)

y_pred_dec_tree_reg = dec_tree_reg.predict(X_test)

"""**Evaluate, and then Predict Image Volume and Calorie Values**"""

# Evaluation of Predicted Volume in terms of all Metrics
mae = MAE(y_test, y_pred_dec_tree_reg)
mse = MSE(y_test, y_pred_dec_tree_reg)
rmse = RMSE(y_test, y_pred_dec_tree_reg)
mape = MAPE(y_test, y_pred_dec_tree_reg)

print('=============================================')
print(f'Predicted Volume Mean Absolute Error: {mae}')
print(f'Predicted Volume Mean Square Error: {mse}')
print(f'Predicted Volume Root Mean Square Error: {rmse}')
print(f'Predicted Volume Mean Absolute Percentage Error: {mape * 100}')
print('==============================================')

countFood = [0] * len(foods)
meanRealVol = [0] * len(foods)
meanEstiamteVol = [0] * len(foods)

meanRealCal = [0] * len(foods)
meanEstiamteCal = [0] * len(foods)

imna = dataset_test['image_name'].tolist()
reVol = y_test.tolist()

pred_calorie = [0] * len(y_pred_dec_tree_reg)

for i in range(len(imna)):

    pred_calorie [i] = test_avDens[i] * test_energy[i] * y_pred_dec_tree_reg[i]
    meanRealCal[imna[i]] += test_realCalori[i]
    meanEstiamteCal[imna[i]] += pred_calorie[i]

    countFood[imna[i]] += 1
    meanRealVol[imna[i]] += reVol[i]
    meanEstiamteVol[imna[i]] += y_pred_dec_tree_reg[i]

# Evaluation of the Predicted Calories in terms of all Metrics
mae = MAE(np.array(test_realCalori), np.array(pred_calorie))
mse = MSE(np.array(test_realCalori), np.array(pred_calorie))
rmse = RMSE(np.array(test_realCalori), np.array(pred_calorie))
mape = MAPE(np.array(test_realCalori), np.array(pred_calorie))

print('=============================================')
print(f'Predicted Calorie Mean Absolute Error: {mae}')
print(f'Predicted Calorie Mean Square Error: {mse}')
print(f'Predicted Calorie Root Mean Square Error: {rmse}')
print(f'Predicted Calorie Mean Absolute Percentage Error: {mape * 100}')
print('==============================================')

"""**Visualize Model Real and Predicted Calorie Results**"""

# Example model outputs as Python arrays
array1 = np.array(y_test)
array2 = np.array(y_pred_dec_tree_reg)
array3 = np.array(test_realCalori)
array4 = np.array(pred_calorie)

# Create a dictionary to store the arrays as key-value pairs
data = {'Real Volume Value (mm^3)': array1, 'DT Model Predicted Volume (mm^3)': array2,
        'Real Calorie Value (kcal)': array3, 'DT Model Predicted Calorie (kcal)': array4}

# Create a Pandas DataFrame from the dictionary
df = pd.DataFrame(data)
print(df.to_markdown())

"""**Visualize the Results**"""

from pandas.plotting  import parallel_coordinates
plt.figure(figsize=(15, 10))
parallel_coordinates(dataset_train,'image_name')
plt.title("Paralel Cordinates Plot")
plt.xlabel('Features')
plt.ylabel('Feature Values')
plt.legend(loc=1,prop={'size':15},frameon=True,shadow=True,facecolor='white',edgecolor='black')
plt.show()

from pandas.plotting import andrews_curves
plt.figure(figsize=(15, 10))
andrews_curves(dataset_train,'image_name')
plt.title("Paralel Cordinates Plot")
plt.legend(loc=1,prop={'size':15},frameon=True,shadow=True,facecolor='white',edgecolor='black')
plt.show()

"""###**Build the RF Model**"""

random_forest_reg = RandomForestRegressor(random_state=0)
random_forest_reg.fit(X_train, y_train)

y_pred_random_forest_reg = random_forest_reg.predict(X_test)

"""**Evaluate, and then Predict Image Volume and Calorie Values**"""

# Evaluation of Predicted Volume in terms of all Metrics
mae = MAE(y_test, y_pred_random_forest_reg)
mse = MSE(y_test, y_pred_random_forest_reg)
rmse = RMSE(y_test, y_pred_random_forest_reg)
mape = MAPE(y_test, y_pred_random_forest_reg)

print('=============================================')
print(f'Predicted Volume Mean Absolute Error: {mae}')
print(f'Predicted Volume Mean Square Error: {mse}')
print(f'Predicted Volume Root Mean Square Error: {rmse}')
print(f'Predicted Volume Mean Absolute Percentage Error: {mape * 100}')
print('==============================================')

countFood = [0] * len(foods)
meanRealVol = [0] * len(foods)
meanEstiamteVol = [0] * len(foods)

meanRealCal = [0] * len(foods)
meanEstiamteCal = [0] * len(foods)

imna = dataset_test['image_name'].tolist()
reVol = y_test.tolist()

pred_calorie = [0] * len(y_pred_random_forest_reg)

for i in range(len(imna)):

    pred_calorie [i] = test_avDens[i] * test_energy[i] * y_pred_random_forest_reg[i]
    meanRealCal[imna[i]] += test_realCalori[i]
    meanEstiamteCal[imna[i]] += pred_calorie[i]

    countFood[imna[i]] += 1
    meanRealVol[imna[i]] += reVol[i]
    meanEstiamteVol[imna[i]] += y_pred_random_forest_reg[i]

# Evaluation of the Predicted Calories in terms of all Metrics
mae = MAE(np.array(test_realCalori), np.array(pred_calorie))
mse = MSE(np.array(test_realCalori), np.array(pred_calorie))
rmse = RMSE(np.array(test_realCalori), np.array(pred_calorie))
mape = MAPE(np.array(test_realCalori), np.array(pred_calorie))

print('=============================================')
print(f'Predicted Calorie Mean Absolute Error: {mae}')
print(f'Predicted Calorie Mean Square Error: {mse}')
print(f'Predicted Calorie Root Mean Square Error: {rmse}')
print(f'Predicted Calorie Mean Absolute Percentage Error: {mape * 100}')
print('==============================================')

"""**Visualize Model Real and Predicted calorie Results**"""

# Example model outputs as Python arrays
array1 = np.array(y_test)
array2 = np.array(y_pred_random_forest_reg)
array3 = np.array(test_realCalori)
array4 = np.array(pred_calorie)

# Create a dictionary to store the arrays as key-value pairs
data = {'Real Volume Value (mm^3)': array1, 'RF Model Predicted Volume (mm^3)': array2,
        'Real Calorie Value (kcal)': array3, 'RF Model Predicted Calorie (kcal)': array4}

# Create a Pandas DataFrame from the dictionary
df = pd.DataFrame(data)
print(df.to_markdown())

"""###**Build the SVM Model**"""

# instantiate and fit SVM to train data
svc = SVR(C=1.0, epsilon=0.2)
clf = svc
clf.fit(X_train, y_train)

# check parameters of SVM
clf.get_params()

y_pred_svc = clf.predict(X_test)

"""**Evaluate, and then Predict Image Volume and Calorie Values**"""

# Evaluation of Predicted Volume in terms of all Metrics
mae = MAE(y_test, y_pred_svc)
mse = MSE(y_test, y_pred_svc)
rmse = RMSE(y_test, y_pred_svc)
mape = MAPE(y_test, y_pred_svc)

print('=============================================')
print(f'Predicted Volume Mean Absolute Error: {mae}')
print(f'Predicted Volume Mean Square Error: {mse}')
print(f'Predicted Volume Root Mean Square Error: {rmse}')
print(f'Predicted Volume Mean Absolute Percentage Error: {mape * 100}')
print('==============================================')

countFood = [0] * len(foods)
meanRealVol = [0] * len(foods)
meanEstiamteVol = [0] * len(foods)

meanRealCal = [0] * len(foods)
meanEstiamteCal = [0] * len(foods)

imna = dataset_test['image_name'].tolist()
reVol = y_test.tolist()

pred_calorie = [0] * len(y_pred_svc)

for i in range(len(imna)):

    pred_calorie [i] = test_avDens[i] * test_energy[i] * y_pred_svc[i]
    meanRealCal[imna[i]] += test_realCalori[i]
    meanEstiamteCal[imna[i]] += pred_calorie[i]

    countFood[imna[i]] += 1
    meanRealVol[imna[i]] += reVol[i]
    meanEstiamteVol[imna[i]] += y_pred_svc[i]

# Evaluation of the Predicted Calories in terms of all Metrics
mae = MAE(np.array(test_realCalori), np.array(pred_calorie))
mse = MSE(np.array(test_realCalori), np.array(pred_calorie))
rmse = RMSE(np.array(test_realCalori), np.array(pred_calorie))
mape = MAPE(np.array(test_realCalori), np.array(pred_calorie))

print('=============================================')
print(f'Predicted Calorie Mean Absolute Error: {mae}')
print(f'Predicted Calorie Mean Square Error: {mse}')
print(f'Predicted Calorie Root Mean Square Error: {rmse}')
print(f'Predicted Calorie Mean Absolute Percentage Error: {mape * 100}')
print('==============================================')

"""**Visualize Model Real and Predicted calorie Results**"""

# Example model outputs as Python arrays
array1 = np.array(y_test)
array2 = np.array(y_pred_svc)
array3 = np.array(test_realCalori)
array4 = np.array(pred_calorie)

# Create a dictionary to store the arrays as key-value pairs
data = {'Real Volume Value (mm^3)': array1, 'SVM Model Predicted Volume (mm^3)': array2,
        'Real Calorie Value (kcal)': array3, 'SVM Model Predicted Calorie (kcal)': array4}

# Create a Pandas DataFrame from the dictionary
df = pd.DataFrame(data)
print(df.to_markdown())

"""###**Evaluate all Models Performance in terms of different Metrics**"""

today = date.today()
DT = [70.061217137772, 58578.77581258899, 242.03052661304727, 19.283757848679606]
RF = [61.64918045092737, 40362.488294178744, 200.90417689579962, 16.11765006181976]
SVM = [131.69196993093632, 125055.40151127253, 353.6317314824456, 36.53613367087476]

N = 4
x = np.arange(N)
c = ['red', 'orange', 'blue']
width = 0.5

# Set position of bar on X axis
r1 = np.arange(0, len(DT) * 3, 3)
r2 = [x + width for x in r1]
r3 = [x + width for x in r2]

plt.figure(figsize=(15, 8))
plt.clf()
#plt.style.use('fivethirtyeight')
plt.ylabel("Error", weight='bold').set_fontsize('17')
plt.bar(r1, DT, width, edgecolor='white', label='Decision Tree')
plt.bar(r2, RF, width, edgecolor='white', label='Random Forest')
plt.bar(r3, SVM, width, edgecolor='white', label='Support Vector Machine')
plt.xlabel('METRICS', weight='bold').set_fontsize('22')
plt.xticks([r + width for r in range(0, len(DT) * 3, 3)], ['MAE', 'MSE', 'RMSE', 'MAPE (%)'], rotation=45, size = 15, fontweight='bold')
plt.title('Performance Comparison of Calorie Predictions Between Models', size = 18, fontweight='bold')
plt.legend(loc = 'best', bbox_to_anchor = (1.0, 1.0), ncol = 1, fontsize = '15',
           fancybox=True, framealpha=1, shadow=True, borderpad=1)
plt.grid(True)
plt.yscale('log')
plt.annotate(f"Bassey Kalu {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.40), textcoords='axes fraction',
             color='black')
plt.savefig('Performance Comparison of Predictions Between Models.png')